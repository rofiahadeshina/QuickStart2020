{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scrapping using python\n",
    "\n",
    "#### References\n",
    "1. [Practical Introduction to Web Scraping in Python](https://realpython.com/python-web-scraping-practical-introduction/)\n",
    "2. [Web Scraping using Python](https://www.datacamp.com/community/tutorials/web-scraping-using-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $ python3 -m venv venv\n",
    "# $ . ./venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.22.0)\n",
      "Requirement already satisfied: BeautifulSoup4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.7.1)\n",
      "Requirement already satisfied: fire in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.3.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests) (1.24.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests) (2019.6.16)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests) (2.8)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from BeautifulSoup4) (1.8)\n",
      "Requirement already satisfied: six in c:\\users\\hp\\anaconda3\\lib\\site-packages (from fire) (1.12.0)\n",
      "Requirement already satisfied: termcolor in c:\\users\\hp\\anaconda3\\lib\\site-packages (from fire) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "#Better\n",
    "#!pip install requests BeautifulSoup4 fire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from requests.exceptions import RequestException\n",
    "from contextlib import closing\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "\n",
    "import fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile ../pyscrap_url.py\n",
    "\n",
    "def simple_get(url):\n",
    "    \"\"\"\n",
    "    Attempts to get the content at `url` by making an HTTP GET request.\n",
    "    If the content-type of response is some kind of HTML/XML, return the\n",
    "    text content, otherwise return None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with closing(get(url, stream=True)) as resp:\n",
    "            if is_good_response(resp):\n",
    "                return resp.content  #.encode(BeautifulSoup.original_encoding)\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "    except RequestException as e:\n",
    "        log_error('Error during requests to {0} : {1}'.format(url, str(e)))\n",
    "        return None\n",
    "\n",
    "\n",
    "def is_good_response(resp):\n",
    "    \"\"\"\n",
    "    Returns True if the response seems to be HTML, False otherwise.\n",
    "    \"\"\"\n",
    "    content_type = resp.headers['Content-Type'].lower()\n",
    "    return (resp.status_code == 200 \n",
    "            and content_type is not None \n",
    "            and content_type.find('html') > -1)\n",
    "\n",
    "\n",
    "def log_error(e):\n",
    "    \"\"\"\n",
    "    It is always a good idea to log errors. \n",
    "    This function just prints them, but you can\n",
    "    make it do anything.\n",
    "    \"\"\"\n",
    "    print(e)\n",
    "    \n",
    "def get_elements(url, tag='',search={}, fname=None):\n",
    "    \"\"\"\n",
    "    Downloads a page specified by the url parameter\n",
    "    and returns a list of strings, one per tag element\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(url,str):\n",
    "        response = simple_get(url)\n",
    "    else:\n",
    "        #if already it is a loaded html page\n",
    "        response = url\n",
    "\n",
    "    if response is not None:\n",
    "        html = BeautifulSoup(response, 'html.parser')\n",
    "        \n",
    "        res = []\n",
    "        if tag:    \n",
    "            for li in html.select(tag):\n",
    "                for name in li.text.split('\\n'):\n",
    "                    if len(name) > 0:\n",
    "                        res.append(name.strip())\n",
    "                       \n",
    "                \n",
    "        if search:\n",
    "            soup = html            \n",
    "            \n",
    "            \n",
    "            r = ''\n",
    "            if 'find' in search.keys():\n",
    "                print('findaing',search['find'])\n",
    "                soup = soup.find(**search['find'])\n",
    "                r = soup\n",
    "\n",
    "                \n",
    "            if 'find_all' in search.keys():\n",
    "                print('findaing all of',search['find_all'])\n",
    "                r = soup.find_all(**search['find_all'])\n",
    "   \n",
    "            if r:\n",
    "                for x in list(r):\n",
    "                    if len(x) > 0:\n",
    "                        res.extend(x)\n",
    "            \n",
    "        return res\n",
    "\n",
    "    # Raise an exception if we failed to get any data from the url\n",
    "    raise Exception('Error retrieving contents at {}'.format(url))    \n",
    "    \n",
    "    \n",
    "if get_ipython().__class__.__name__ == '__main__':\n",
    "    fire(get_tag_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = get_elements('https://africafreak.com/100-most-influential-twitter-users-in-africa', tag='h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "res\n",
    "#print(res)\n",
    "for i in res[100:]:\n",
    "    res.remove(i)\n",
    "    #print(res)\n",
    "names_infl = []\n",
    "handle_infl = []\n",
    "for r in res:\n",
    "    split_data = r.split('.',maxsplit=1)[1].rsplit('(',maxsplit=1)\n",
    "    name = split_data[0].split(',')[0].strip()\n",
    "    handle =  split_data[1].split(')',maxsplit=1)[0]\n",
    "    names_infl.append(name)\n",
    "    handle_infl.append(handle)\n",
    "    #print(f'{name}:{handle}'\n",
    "#print(handle_infl)\n",
    "#df_influencer_name = pd.DataFrame(names_infl)\n",
    "#df_influencer_handle = pd.DataFrame(handle_infl)\n",
    "#df_influencer_name.to_csv('C:/Users/HP/Desktop/CV, P.Statement and others/10 Academy/Topinfluencer_handle.csv', index = False)\n",
    "\n",
    "#print(df_influencer_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url= 'https://www.atlanticcouncil.org/blogs/africasource/african-leaders-respond-to-coronavirus-on-twitter/#east-africa'\n",
    "response = simple_get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@EswatiniGovern1', '@MalawiGovt', '@hagegeingob', '@FinanceSC', '@PresidencyZA', '@mohzambia', '@edmnangagwa', '@MinSantedj', '@hawelti', '@StateHouseKenya', '@PaulKagame', '@M_Farmaajo', '@SouthSudanGov', '@SudanPMHamdok', '@TZSpokesperson', '@KagutaMuseveni', '@angola_Mirex', '@willynyamitwe', '@Cherif_MZ', '@Presidence_RDC', '@PresidentABO', '@PresidenceBenin', '@rochkaborepf', '@PresidenciaCV', '@AOuattara_PRCI', '@Presidency_GMB', '@NAkufoAddo', '@President_GN', '@USEmbalo', '@PresidenceMali', '@CheikhGhazouani', '@IssoufouMhm', '@MBuhari', '@Macky_Sall', '@PresidentBio', '@MSPS_Togo']\n"
     ]
    }
   ],
   "source": [
    "url= 'https://www.atlanticcouncil.org/blogs/africasource/african-leaders-respond-to-coronavirus-on-twitter/#east-africa'\n",
    "response = get(url).content\n",
    "re_gov = get_elements(response, tag='blockquote')\n",
    "names = []\n",
    "handles = []\n",
    "for r in re_gov:\n",
    "    split_data = r.split('‚Äî ',maxsplit=1)[1].rsplit('(',maxsplit=1)\n",
    "    #print(split_data)\n",
    "    name = split_data[0].split(',')[0].strip()\n",
    "    handle =  split_data[1].rsplit(')',maxsplit=1)[0]\n",
    "    names.append(name)\n",
    "    handles.append(handle)\n",
    "\n",
    "nam_handle = f'{name}:{handle}'\n",
    "print(handles)\n",
    "#print(df_gvrn)\n",
    "#res[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gvrn = pd.DataFrame(handles, columns=[\"Twitter Handles\"])\n",
    "df_gvrn.to_csv (r'C:\\Users\\HP\\Desktop\\CV, P.Statement and others\\10 Academy\\gov_influencers_handles.csv', index = False, header=True)\n",
    "#print(df_gvrn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "#sns.set()\n",
    "\n",
    "\n",
    "# to view all columns\n",
    "#pd.set_option(\"display.max.columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tweepy\n",
    "import tweepy\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = \"re8a7dLArl8y2Tic312dbRWxH\"\n",
    "consumer_secret = \"DcARRXgLiCfFE0y1aBFF9WlfHtLzZw2TgOCEOkM31rdKG8rJD0\"\n",
    "access_token = \"994261917253472256-bRMXtnLl19sVm2a3flZv6t2WaW9b1AG\"\n",
    "access_token_secret = \"HMnwkCLauS0OLOVNi62Ay1K9OcVlEvgawcfqOSuIIHbJz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting tweets for 100 influencers\n",
    "tweets = []\n",
    "tweetCount=1\n",
    "for i in handle_infl:\n",
    "    try:\n",
    "        results=api.user_timeline(id=i, count=tweetCount)\n",
    "    except tweepy.TweepError as e:\n",
    "                continue  \n",
    "    for tweet in results:\n",
    "        tweets.append(tweet.text)\n",
    "#print(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    Influencer Tweets\n",
      "0   What an inspiring interchange. Way to go Luke!...\n",
      "1                 WE ARE LIVE https://t.co/vB1hS6EmZj\n",
      "2      La video yase-butcher! https://t.co/6ZBC9vIs2X\n",
      "3   üìå [JOB OPPORTUNITY] We're hiring an Info Finde...\n",
      "4   RT @LukwesaBurak: Please watch this if you hav...\n",
      "5   RT @A4A_Internet: The stakes are too high to k...\n",
      "6   RT @globalcompact: Where other people saw a pr...\n",
      "7   Our SEO wizard team located at Vancouver is re...\n",
      "8   RT @YvonneNdege: The economic hit on remittanc...\n",
      "9   He taught me to be kind but honest, open minde...\n",
      "10  RT @gpgSocDev: MEC for @gpgSocDev Nomathemba M...\n",
      "11  Instead of hosting its popular markets, @Kamer...\n",
      "12  In the last few days, tens of thousands of peo...\n",
      "13  RT @CBSNews: As the coronavirus pandemic cripp...\n",
      "14  As a continent, Africa will experience a major...\n",
      "15  RT @African_Markets: Airtel makes its debut as...\n",
      "16  RT @KenRoth: The UN envoy in Yemen calls for a...\n",
      "17  https://t.co/EfS2EsKmZ4 #Art #ArtistOfTheMonth...\n",
      "18                  Love this https://t.co/mrXkd2rcHD\n",
      "19  Met‚Äôs former head of diversity says it‚Äôs ‚Äòworr...\n",
      "20                            RT @thenodshow: #TheNod\n",
      "21  @Jonathan_Witt We don't agree on much, but we'...\n",
      "22  RT @umunyana_rugege: ‚ÄúHunger is not an issue o...\n",
      "23  RIP Ennio Morricone. His music made this scene...\n",
      "24  RT @BBCBreakfast: ‚ÅâÔ∏è Some questions are best a...\n",
      "25                          @vanpopeROCKS Go for it üôÉ\n",
      "26  @JRYDERPSA Hi @JRYDERPSA. We appreciate your i...\n",
      "27  üëÄ EXCLUSIVE SNEAK PEEK - Check out the first l...\n",
      "28  Repentant bandits in Nigeria's north-western s...\n",
      "29  RT @chriscantino: If you‚Äôre bootstrapped, don‚Äô...\n",
      "..                                                ...\n",
      "62  RT @GadgetZA: After a 7-year wait, @Microsoft‚Äô...\n",
      "63  RT @thejulianbass: if y‚Äôall can retweet this e...\n",
      "64  RT @MelissaMaykin: Talking #PressFreedom and D...\n",
      "65  RT @MeteoKenya: ‚ö†Ô∏è Strong winds and large wave...\n",
      "66  Nelson Mandela stands as an inspiration to all...\n",
      "67  CELEBRATING THE LEGACY OF NELSON MANDELA https...\n",
      "68  This Mandela Day, let‚Äôs all take action, inspi...\n",
      "69  [Live] Follow the 19th Weekly Press Briefing o...\n",
      "70  Tonight @etv @eNCA 17:30\\n@Devi_HQ and I have ...\n",
      "71  @SmithInAfrica is now @SmithInAmerica. Hope to...\n",
      "72  RT @BrenthurstF: Talks between Ethiopia and Eg...\n",
      "73  Here are your Top 20 Record of The Year nomine...\n",
      "74  Couch ‚úÖ  Popcorn ‚úÖ Blanket ‚úÖ\\n\\n‚Ä¶what‚Äôs missin...\n",
      "75  RT @StefanT15: Amazing thanks again....and on ...\n",
      "76  Thrilled that CAMFED Association leader Ruth j...\n",
      "77  The best weather station reviews for your home...\n",
      "78  Twitter stories üíî Fleets... https://t.co/TePnv...\n",
      "79  RT @LemuelIrabor: Roadside bushes hide more be...\n",
      "80  Together we stand in solidarity for equality @...\n",
      "81  RT @ONEinAfrica: üëÄ EXCLUSIVE SNEAK PEEK - Chec...\n",
      "82  Looking for something exciting and fun to do w...\n",
      "83  RT @CarmenComedian: I love cats but this is al...\n",
      "84  He's not just one of our fave new rappers, the...\n",
      "85  RT @ToroxaD: Kaise KAI gangan/Thank you to @si...\n",
      "86  @zilevandamme I meant that I believe you, and ...\n",
      "87  RT @AdvoBarryRoux: Lesego Mogakabe she went mi...\n",
      "88  Cut and paste: what‚Äôs the (T)Racing Point? | @...\n",
      "89  RT @TechCrunch: YouTube Music is now free with...\n",
      "90  RT @CliffCentralCom: On The @GarethCliff Show ...\n",
      "91  Actual video footage of me starting off the we...\n",
      "\n",
      "[92 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "#to convert tweets to csv\n",
    "influencer_tweets = pd.DataFrame(tweets, columns=[\"Influencer Tweets\"] )\n",
    "print(influencer_tweets)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting followers for 100 influencers\n",
    "# Calling the get_user function with our parameters\n",
    "followers = []\n",
    "for i in handle_infl:\n",
    "    try:\n",
    "        results = api.get_user(id=i)\n",
    "    except tweepy.TweepError as e:\n",
    "        continue \n",
    "    followers.append(results.followers_count)\n",
    "    #print(results.followers_count)\n",
    "#print(followers)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Number of followers\n",
      "0                25701\n",
      "1                31288\n",
      "2               101276\n",
      "3                68070\n",
      "4                21963\n"
     ]
    }
   ],
   "source": [
    "# followers dataframe\n",
    "df_followers = pd.DataFrame(followers, columns = [\"Number of followers\"])\n",
    "df_followers.to_csv('C:/Users/HP/Desktop/CV, P.Statement and others/10 Academy/infl_followers.csv', index = False)\n",
    "print(df_followers.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting no of likes for 100 influencers\n",
    "likes = []\n",
    "for i in handle_infl:\n",
    "    try:\n",
    "        results = api.get_user(id=i)\n",
    "    except tweepy.TweepError as e:\n",
    "        continue\n",
    "    likes.append(results.favourites_count)\n",
    "    #print(results.followers_count)\n",
    "#print(likes)\n",
    "\n",
    "df_likes = pd.DataFrame(likes, columns=[\"No of likes\"])\n",
    "df_likes.to_csv (r'C:\\Users\\HP\\Desktop\\CV, P.Statement and others\\10 Academy\\top_influencers_likes.csv', index = False, header=True)\n",
    "#print(df_likes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37, 3059, 2840, 4591, 5048, 2876, 179, 0, 1732, 366, 2927, 1819, 2138, 893, 294, 172, 2988, 132, 425, 659, 2257, 813, 749, 941, 3605, 15, 0, 1155, 89620, 220, 1643, 49690, 151, 2416, 653, 1923, 1979, 983, 0, 1905, 3123, 1345, 3706, 4615, 406, 1008, 1143, 61980, 1114, 757, 1487, 657, 391, 1940, 25646, 5472, 3166, 1545, 74081, 10830, 141, 128, 11, 3348, 480, 987, 561, 1641, 2204, 985, 186, 305, 0, 455, 1618, 269, 2988, 25601, 0, 65, 530362, 1999, 109, 1223, 3948, 411, 478, 2304, 652, 631, 14, 356, 325]\n"
     ]
    }
   ],
   "source": [
    "# getting no of following for 100 influencers\n",
    "following = []\n",
    "for i in handle_infl:\n",
    "    try:\n",
    "        results = api.get_user(id=i)\n",
    "    except tweepy.TweepError as e:\n",
    "        continue\n",
    "    following.append(results.friends_count)\n",
    "\n",
    "    \n",
    "print(following)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Number of following\n",
      "0                   37\n",
      "1                 3059\n",
      "2                 2840\n",
      "3                 4591\n",
      "4                 5048\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# followers dataframe\n",
    "d_following = pd.DataFrame(following, columns = [\"Number of following\"])\n",
    "d_following.to_csv('C:/Users/HP/Desktop/CV, P.Statement and others/10 Academy/following.csv', index = False)\n",
    "print(d_following.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Empty DataFrame\n",
      "Columns: [No of retweets]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#getting retweets\n",
    "no_of_retweets = []\n",
    "for x in handle_infl:\n",
    "    try:\n",
    "        tweets = tweepy.Cursor(api.user_timeline, id=x, lang=\"en\").items()\n",
    "        for tweet in tweets:\n",
    "            no_of_retweets.append(tweet.retweet_count)\n",
    "            #print(tweet.retweet_count)\n",
    "    except tweepy.TweepError as e:\n",
    "        continue\n",
    "print(no_of_retweets)\n",
    "\n",
    "#retweets dataframe\n",
    "df_retweets = pd.DataFrame(no_of_retweets, columns=[\"No of retweets\"])\n",
    "df_retweets.to_csv (r'C:\\Users\\HP\\Desktop\\CV, P.Statement and others\\10 Academy\\finalJ_retweets.csv', index = False, header=True)\n",
    "print(df_retweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11185\n"
     ]
    }
   ],
   "source": [
    "#getting no of tweets shared\n",
    "no_tweets_shared = []\n",
    "for i in handle_infl:\n",
    "    try:\n",
    "        results = api.get_user(id=i)\n",
    "    except tweepy.TweepError as e:\n",
    "        continue\n",
    "    no_tweets_shared.append(results.statuses_count)\n",
    "    \n",
    "print(no_tweets_shared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Popularity reach\n",
    "### Reach Score\n",
    "### Relevance Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#popularity reach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Number of following  Number of followers\n",
      "0                   37                25701\n",
      "1                 3059                31288\n",
      "2                 2840               101276\n",
      "3                 4591                68070\n",
      "4                 5048                21963\n",
      "   Number of following  Number of followers  reach_score\n",
      "0                   37                25701        25664\n",
      "1                 3059                31288        28229\n",
      "2                 2840               101276        98436\n",
      "3                 4591                68070        63479\n",
      "4                 5048                21963        16915\n"
     ]
    }
   ],
   "source": [
    "#Reach Score = followers - following\n",
    "reach = pd.concat([df_followers,d_following], axis=1)\n",
    "print(reach.head())\n",
    "reach['reach_score']= reach[\"Number of followers\"] - reach[\"Number of following\"]\n",
    "print(reach.head()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets=[]\n",
    "for i in id_num:\n",
    "    try:\n",
    "        results = api.retweets(id=i,count=10)\n",
    "    except tweepy.TweepError as e:\n",
    "        continue\n",
    "    #print(results)\n",
    "    #retweets.append(results.text)\n",
    "#print(retweets)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hash_tags\n",
    "hash_tags = []\n",
    "for i in handle_infl:\n",
    "    try:\n",
    "        results = api.get_user(id=i)\n",
    "    except tweepy.TweepError as e:\n",
    "        continue\n",
    "    if hasattr(status, \"entities\"):\n",
    "        entities = status.entities\n",
    "        # get hashtags\n",
    "        if \"hashtags\" in entities:\n",
    "            for ent in entities[\"hashtags\"]:\n",
    "                if ent is not None:\n",
    "                    if \"text\" in ent:\n",
    "                        hashtag = ent[\"text\"]\n",
    "                        if hashtag is not None:\n",
    "                            hashtags.append(hashtag)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "for status in Cursor(auth_api.user_timeline, id=handle).items():\n",
    "                tweet_count+= 1\n",
    "                if hasattr(status, \"entities\"):\n",
    "                    entities = status.entities\n",
    "                # get hashtags\n",
    "                if \"hashtags\" in entities:\n",
    "                    for ent in entities[\"hashtags\"]:\n",
    "                        if ent is not None:\n",
    "                            if \"text\" in ent:\n",
    "                                hashtag = ent[\"text\"]\n",
    "                                if hashtag is not None:\n",
    "                                    hashtags.append(hashtag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting follower for gov\n",
    "followers = []\n",
    "for i in name:\n",
    "    results = api.get_user(id=i)\n",
    "    followers.append(results.followers_count)\n",
    "    #print(results.followers_count)\n",
    "#print(followers)\n",
    "\n",
    "# followers dataframe\n",
    "#df_followers = pd.DataFrame(followers, columns = [\"Number of followers\"])\n",
    "#df_followers.to_csv('C:/Users/HP/Desktop/CV, P.Statement and others/10 Academy/gov_followers.csv', index = False)\n",
    "#print(df_followers.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Twitter Handles\n",
      "0                61\n",
      "1               915\n",
      "2               268\n",
      "3                12\n",
      "4                60\n",
      "5               163\n",
      "6                65\n",
      "7               587\n",
      "8               731\n",
      "9                61\n",
      "10              616\n",
      "11               22\n",
      "12              348\n",
      "13               43\n",
      "14                1\n",
      "15               76\n",
      "16              447\n",
      "17             6258\n",
      "18              466\n",
      "19               98\n",
      "20               16\n",
      "21               94\n",
      "22              475\n",
      "23               91\n",
      "24                4\n",
      "25              452\n",
      "26              133\n",
      "27                6\n",
      "28             6068\n",
      "29             1732\n",
      "30                0\n",
      "31                3\n",
      "32                8\n",
      "33              530\n",
      "34                0\n",
      "35                1\n"
     ]
    }
   ],
   "source": [
    "# The Twitter users who we want to get tweets from\n",
    "name = handles\n",
    "# Calling the get_user function with our parameters\n",
    "\n",
    "\n",
    "#print(tweet.retweet_count)\n",
    "#print(tweet.favorite_count)\n",
    "\n",
    "#no of likes\n",
    "likes = []\n",
    "for i in name:\n",
    "    results = api.get_user(id=i)\n",
    "    likes.append(results.favourites_count)\n",
    "    #print(results.followers_count)\n",
    "#print(likes)\n",
    "df_likes = pd.DataFrame(likes, columns=[\"Twitter Handles\"])\n",
    "df_likes.to_csv (r'C:\\Users\\HP\\Desktop\\CV, P.Statement and others\\10 Academy\\gov_influencers_likes.csv', index = False, header=True)\n",
    "#print(df_likes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-31-8e67371b035e>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-31-8e67371b035e>\"\u001b[1;36m, line \u001b[1;32m11\u001b[0m\n\u001b[1;33m    # retweet_count.append(retweets)\u001b[0m\n\u001b[1;37m                                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "#no of retweets\n",
    "name = handles\n",
    "retweets = []\n",
    "for i in name:\n",
    "    results = api.user_timeline(id=i)\n",
    "    retweets.\n",
    "    #print(results)\n",
    "    if hasattr(results, 'retweet_count'):\n",
    "        retweets = results.retweet_count\n",
    "        #if retweets is not None:\n",
    "           # retweet_count.append(retweets)\n",
    "\n",
    "print(retweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[82, 26, 55, 224, 14, 95, 116, 127, 434, 214, 181, 2, 463, 115, 32, 28, 312, 4655, 195, 125, 4, 66, 151, 885, 23, 27, 352, 29, 181, 1001, 9, 17, 26, 171, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "name = handles\n",
    "following = []\n",
    "for i in name:\n",
    "    results = api.get_user(id=i)\n",
    "    following.append(results.friends_count)\n",
    "    \n",
    "print(following)\n",
    "        \n",
    "df_ffl = pd.DataFrame(following, columns=[\"Number of following\"])\n",
    "df_ffl.to_csv (r'C:\\Users\\HP\\Desktop\\CV, P.Statement and others\\10 Academy\\gov_influencers_following.csv', index = False, header=True)\n",
    "#print(df_ffl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Twitter Handles  Number of following\n",
      "0   @EswatiniGovern1                   82\n",
      "1        @MalawiGovt                   26\n",
      "2       @hagegeingob                   55\n",
      "3         @FinanceSC                  224\n",
      "4      @PresidencyZA                   14\n",
      "5         @mohzambia                   95\n",
      "6       @edmnangagwa                  116\n",
      "7        @MinSantedj                  127\n",
      "8           @hawelti                  434\n",
      "9   @StateHouseKenya                  214\n",
      "10       @PaulKagame                  181\n",
      "11       @M_Farmaajo                    2\n",
      "12    @SouthSudanGov                  463\n",
      "13    @SudanPMHamdok                  115\n",
      "14   @TZSpokesperson                   32\n",
      "15   @KagutaMuseveni                   28\n",
      "16     @angola_Mirex                  312\n",
      "17    @willynyamitwe                 4654\n",
      "18        @Cherif_MZ                  195\n",
      "19   @Presidence_RDC                  125\n",
      "20     @PresidentABO                    4\n",
      "21  @PresidenceBenin                   66\n",
      "22     @rochkaborepf                  151\n",
      "23    @PresidenciaCV                  885\n",
      "24   @AOuattara_PRCI                   23\n",
      "25   @Presidency_GMB                   27\n",
      "26       @NAkufoAddo                  352\n",
      "27     @President_GN                   29\n",
      "28         @USEmbalo                  181\n",
      "29   @PresidenceMali                 1001\n",
      "30  @CheikhGhazouani                    9\n",
      "31      @IssoufouMhm                   17\n",
      "32          @MBuhari                   26\n",
      "33       @Macky_Sall                  171\n",
      "34     @PresidentBio                    0\n",
      "35        @MSPS_Togo                    2\n"
     ]
    }
   ],
   "source": [
    "reach_score = pd.concat([df_gvrn,df_ffl], axis=1)\n",
    "#print(reach_score)\n",
    "#reach_score.sort_order()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(api.retweets(197493438))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the API object while passing in auth information\n",
    "# The Twitter user who we want to get tweets from\n",
    "name = handles\n",
    "# Calling the get_user function with our parameters\n",
    "followers = []\n",
    "for i in name:\n",
    "    results = api.get_user(id=i)\n",
    "   # print(results)\n",
    "    #followers.append(results.followers_count)\n",
    "    #print(results.followers_count)\n",
    "#print(followers)\n",
    "\n",
    "# followers dataframe\n",
    "#df_followers = pd.DataFrame(followers, columns = [\"Number of followers\"])\n",
    "#df_followers.to_csv('C:/Users/HP/Desktop/CV, P.Statement and others/10 Academy/gov_followers.csv', index = False)\n",
    "#print(df_followers.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scrapping using bash script\n",
    "If the web site has a quite simple HTML, you can easily use curl to perform the request and then extract the needed values using bash commands grep, cut , sed, ..\n",
    "\n",
    "This tutorial is adapted from [this](https://medium.com/@LiliSousa/web-scraping-with-bash-690e4ee7f98d) medium article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "# curl the page and save content to tmp_file\n",
    "#url = \"https://www.atlanticcouncil.org/blogs/africasource/african-leaders-respond-to-coronavirus-on-twitter/#east-africa\"\n",
    "#curl -X GET $url -o tmp_file\n",
    "\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "# write headers to CSV file\n",
    "echo \"Name, twitter_id\" >> extractData.csv\n",
    "n=\"1\"\n",
    "while [ $n -lt 2 ]\n",
    "do\n",
    "  \n",
    "  #get title\n",
    "  title=$(cat tmp_file | grep \"class=\\\"twitter-tweet\\\"\" | cut -d ';' -f1 )\n",
    "  echo $title\n",
    "  #get author\n",
    "  #twitter_id=$(cat tmp_file |grep -A1 \"class=\\\"css-901oao css-16my406 r-1qd0xha r-ad9z0x r-bcqeeo r-qvutc0\\\"\" | tail -1)\n",
    "\n",
    "  #echo \"$title, $twitter_id\" >> extractData.csv\n",
    "  #echo \"$title, $twitter_id\"\n",
    "    \n",
    "  n=$[$n+1]\n",
    "\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
