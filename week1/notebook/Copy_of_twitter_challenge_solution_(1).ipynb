{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rziXcUSBskS3"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wbw8nRiW2le6"
   },
   "source": [
    "# Twitter Developer Account\n",
    "In order to use Twitter’s API, we have to create a developer account on the Twitter apps site.\n",
    " * Log in or make a Twitter account at https://apps.twitter.com/.\n",
    " * Create a new app (button on the top right)\n",
    " \n",
    "<img src=https://miro.medium.com/max/1400/0*Dq78m3JKoSqZY5SS.png style=\"width: 200px;\">\n",
    "\n",
    "Fill in the app creation page with a unique name, a website name (use a placeholder website if you don’t have one), and a project description. Accept the terms and conditions and proceed to the next page.\n",
    "\n",
    "Once your project has been created, click on the “Keys and Access Tokens” tab. You should now be able to see your consumer secret and consumer key.\n",
    "\n",
    "<img src=https://miro.medium.com/max/1400/0*YU1pFqTw6Dn-ZmOd.png style=\"width: 200px;\">\n",
    "\n",
    "You’ll also need a pair of access tokens. Scroll down and request those tokens. The page should refresh, and you should now have an access token and access token secret.\n",
    "\n",
    "<img src=https://miro.medium.com/max/1400/0*_gnOgA0aaAqPgDJG.png style=\"width: 200px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9xTtIH2m2le8"
   },
   "source": [
    "## Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UI2fIQFxrNLB",
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "\n",
    "# to view all columns\n",
    "pd.set_option(\"display.max.columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "colab_type": "code",
    "id": "3pf5Xapqrq0M",
    "outputId": "7650ebcc-fc9b-45b4-d41b-c3798e99eeb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy) (1.3.0)\n",
      "Requirement already satisfied: requests>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy) (2.23.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy) (1.12.0)\n",
      "Requirement already satisfied: PySocks>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from tweepy) (1.7.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy) (3.0.4)\n",
      "Requirement already satisfied: textblob in /usr/local/lib/python3.6/dist-packages (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.6/dist-packages (from textblob) (3.2.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.1->textblob) (1.12.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Requirement already satisfied: tweet-preprocessor in /usr/local/lib/python3.6/dist-packages (0.6.0)\n"
     ]
    }
   ],
   "source": [
    "#Import the necessary methods from tweepy library  \n",
    "\n",
    "#install tweepy if you don't have it\n",
    "!pip install tweepy\n",
    "import tweepy\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "\n",
    "#sentiment analysis package\n",
    "!pip install textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "#general text pre-processor\n",
    "!pip install nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "\n",
    "#tweet pre-processor \n",
    "!pip install tweet-preprocessor\n",
    "import preprocessor as p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N3ciJ47f2lfZ"
   },
   "source": [
    "### Starting code\n",
    "Below we define some starting codes (python classes and function) to illustrate and assist on how to fetch data from twitter and analyse them. \n",
    "\n",
    "### **Your task is**\n",
    "1. Go through the code and understand it. Know what each function does\n",
    "2. If you find error, fix it. Ask for help in the slack channel if you find serious mistake\n",
    "3. Extend the code such that it will be useful for topics you choose to analyse\n",
    "4. Make nice plots and share your finding (e.g.  insight on the main covid19 twitter converstions about your country)\n",
    "5. Submit what ever you managed to do by Wednesday morning. But you should keep using what you build to write blogs, share on facebook, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M8BihyYl2lfd"
   },
   "outputs": [],
   "source": [
    "class tweetsearch():\n",
    "    '''\n",
    "    This is a basic class to search and download twitter data.\n",
    "    You can build up on it to extend the functionalities for more \n",
    "    sophisticated analysis\n",
    "    '''\n",
    "    def __init__(self, cols=None,auth=None):\n",
    "        #\n",
    "        if not cols is None:\n",
    "            self.cols = cols\n",
    "        else:\n",
    "            self.cols = ['id', 'created_at', 'source', 'original_text','clean_text', \n",
    "                    'sentiment','polarity','subjectivity', 'lang',\n",
    "                    'favorite_count', 'retweet_count', 'original_author',   \n",
    "                    'possibly_sensitive', 'hashtags',\n",
    "                    'user_mentions', 'place', 'place_coord_boundaries']\n",
    "            \n",
    "        if auth is None:\n",
    "            \n",
    "            #Variables that contains the user credentials to access Twitter API \n",
    "             #Variables that contains the user credentials to access Twitter API \n",
    "            consumer_key = \"secret \"\n",
    "            consumer_secret = \"secret\"\n",
    "            access_token = \"secret\"\n",
    "            access_token_secret = \"secret\"\n",
    "            \n",
    "\n",
    "\n",
    "            #This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "            auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "            auth.set_access_token(access_token, access_token_secret)\n",
    "            \n",
    "\n",
    "        #            \n",
    "        self.auth = auth\n",
    "        self.api = tweepy.API(auth) \n",
    "        self.filtered_tweet = ''\n",
    "            \n",
    "\n",
    "    def clean_tweets(self, twitter_text):\n",
    "\n",
    "        #use pre processor\n",
    "        tweet = p.clean(twitter_text)\n",
    "\n",
    "         #HappyEmoticons\n",
    "        emoticons_happy = set([\n",
    "            ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "            ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "            '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "            'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "            '<3'\n",
    "            ])\n",
    "\n",
    "        # Sad Emoticons\n",
    "        emoticons_sad = set([\n",
    "            ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "            ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "            ':c', ':{', '>:\\\\', ';('\n",
    "            ])\n",
    "\n",
    "        #Emoji patterns\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                 u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                 u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                 u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                 u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                 u\"\\U00002702-\\U000027B0\"\n",
    "                 u\"\\U000024C2-\\U0001F251\"\n",
    "                 \"]+\", flags=re.UNICODE)\n",
    "\n",
    "        #combine sad and happy emoticons\n",
    "        emoticons = emoticons_happy.union(emoticons_sad)\n",
    "\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        word_tokens = nltk.word_tokenize(tweet)\n",
    "        #after tweepy preprocessing the colon symbol left remain after      \n",
    "        #removing mentions\n",
    "        tweet = re.sub(r':', '', tweet)\n",
    "        tweet = re.sub(r'‚Ä¶', '', tweet)\n",
    "\n",
    "        #replace consecutive non-ASCII characters with a space\n",
    "        tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "\n",
    "        #remove emojis from tweet\n",
    "        tweet = emoji_pattern.sub(r'', tweet)\n",
    "\n",
    "        #filter using NLTK library append it to a string\n",
    "        filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "        #looping through conditions\n",
    "        filtered_tweet = []    \n",
    "        for w in word_tokens:\n",
    "        #check tokens against stop words , emoticons and punctuations\n",
    "            if w not in stop_words and w not in emoticons and w not in string.punctuation:\n",
    "                filtered_tweet.append(w)\n",
    "\n",
    "        return ' '.join(filtered_tweet)            \n",
    "\n",
    "    def get_tweets(self, keyword, csvfile=None):\n",
    "        \n",
    "        \n",
    "        df = pd.DataFrame(columns=self.cols)\n",
    "        \n",
    "        if not csvfile is None:\n",
    "            #If the file exists, then read the existing data from the CSV file.\n",
    "            if os.path.exists(csvfile):\n",
    "                df = pd.read_csv(csvfile, header=0)\n",
    "            \n",
    "\n",
    "        #page attribute in tweepy.cursor and iteration\n",
    "        for page in tweepy.Cursor(self.api.search, q=keyword,count=200, include_rts=False).pages():\n",
    "\n",
    "            # the you receive from the Twitter API is in a JSON format and has quite an amount of information attached\n",
    "            for status in page:\n",
    "                \n",
    "                new_entry = []\n",
    "                status = status._json\n",
    "                \n",
    "                #filter by language\n",
    "                if status['lang'] != 'en':\n",
    "                    continue\n",
    "\n",
    "                \n",
    "                #if this tweet is a retweet update retweet count\n",
    "                if status['created_at'] in df['created_at'].values:\n",
    "                    i = df.loc[df['created_at'] == status['created_at']].index[0]\n",
    "                    #\n",
    "                    cond1 = status['favorite_count'] != df.at[i, 'favorite_count']\n",
    "                    cond2 = status['retweet_count'] != df.at[i, 'retweet_count']\n",
    "                    if cond1 or cond2:\n",
    "                        df.at[i, 'favorite_count'] = status['favorite_count']\n",
    "                        df.at[i, 'retweet_count'] = status['retweet_count']\n",
    "                    continue\n",
    "\n",
    "                #calculate sentiment\n",
    "                filtered_tweet = self.clean_tweets(status['text'])\n",
    "                blob = TextBlob(filtered_tweet)\n",
    "                Sentiment = blob.sentiment     \n",
    "                polarity = Sentiment.polarity\n",
    "                subjectivity = Sentiment.subjectivity\n",
    "\n",
    "                new_entry += [status['id'], status['created_at'],\n",
    "                              status['source'], status['text'], filtered_tweet, \n",
    "                              Sentiment,polarity,subjectivity, status['lang'],\n",
    "                              status['favorite_count'], status['retweet_count']]\n",
    "\n",
    "                new_entry.append(status['user']['screen_name'])\n",
    "\n",
    "                try:\n",
    "                    is_sensitive = status['possibly_sensitive']\n",
    "                except KeyError:\n",
    "                    is_sensitive = None\n",
    "\n",
    "                new_entry.append(is_sensitive)\n",
    "\n",
    "                hashtags = \", \".join([hashtag_item['text'] for hashtag_item in status['entities']['hashtags']])\n",
    "                new_entry.append(hashtags) #append the hashtags\n",
    "\n",
    "                #\n",
    "                mentions = \", \".join([mention['screen_name'] for mention in status['entities']['user_mentions']])\n",
    "                new_entry.append(mentions) #append the user mentions\n",
    "\n",
    "                try:\n",
    "                    xyz = status['place']['bounding_box']['coordinates']\n",
    "                    coordinates = [coord for loc in xyz for coord in loc]\n",
    "                except TypeError:\n",
    "                    coordinates = None\n",
    "                #\n",
    "                new_entry.append(coordinates)\n",
    "\n",
    "                try:\n",
    "                    location = status['user']['location']\n",
    "                except TypeError:\n",
    "                    location = ''\n",
    "                #\n",
    "                new_entry.append(location)\n",
    "\n",
    "                #now append a row to the dataframe\n",
    "                single_tweet_df = pd.DataFrame([new_entry], columns=self.cols)\n",
    "                df = df.append(single_tweet_df, ignore_index=True)\n",
    "\n",
    "        if not csvfile is None:\n",
    "            #save it to file\n",
    "            df.to_csv(csvfile, columns=self.cols, index=False, encoding=\"utf-8\")\n",
    "            \n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1jJuOnyR2lfm"
   },
   "source": [
    "### Search twitter and fetch data example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "6U3IyY4x2lft",
    "outputId": "8f8cbaa6-d839-4f82-9bf5-152d229a609a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "covid_keywords = '#COVID19Ethiopia'  #hashtag based search\n",
    "tweets_file = 'ethiopia_covid19_1july2020.json'\n",
    "\n",
    "\n",
    "##get data on keywords\n",
    "if os.path.exists(tweets_file):\n",
    "    #get file if you have already downloaded what you wanted\n",
    "    df = pd.read_csv(tweets_file, header=0)\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = df.created_at.map(pd.Timestamp)\n",
    "        df = df.sort_values('timestamp').set_index('timestamp')\n",
    "        df = df.drop('id',axis=1)    \n",
    "else:\n",
    "    ts = tweetsearch()\n",
    "    df = ts.get_tweets(covid_keywords, csvfile=tweets_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "colab_type": "code",
    "id": "VglbamsX2lf3",
    "outputId": "bf0377ae-54b0-45c6-bdd9-e48f09a3da84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 74 entries, 0 to 73\n",
      "Data columns (total 17 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   id                      74 non-null     object \n",
      " 1   created_at              74 non-null     object \n",
      " 2   source                  74 non-null     object \n",
      " 3   original_text           74 non-null     object \n",
      " 4   clean_text              74 non-null     object \n",
      " 5   sentiment               74 non-null     object \n",
      " 6   polarity                74 non-null     float64\n",
      " 7   subjectivity            74 non-null     float64\n",
      " 8   lang                    74 non-null     object \n",
      " 9   favorite_count          74 non-null     object \n",
      " 10  retweet_count           74 non-null     object \n",
      " 11  original_author         74 non-null     object \n",
      " 12  possibly_sensitive      33 non-null     object \n",
      " 13  hashtags                74 non-null     object \n",
      " 14  user_mentions           74 non-null     object \n",
      " 15  place                   0 non-null      object \n",
      " 16  place_coord_boundaries  74 non-null     object \n",
      "dtypes: float64(2), object(15)\n",
      "memory usage: 10.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "colab_type": "code",
    "id": "51RaE-xj2lgE",
    "outputId": "c9dd3b7c-1d8b-45e2-9410-bfe0e68ace9e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>source</th>\n",
       "      <th>original_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>lang</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>original_author</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>user_mentions</th>\n",
       "      <th>place</th>\n",
       "      <th>place_coord_boundaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1283201041878704134</td>\n",
       "      <td>Wed Jul 15 00:45:43 +0000 2020</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>RT @ETHealthData: #COVID19Ethiopia updates \\nD...</td>\n",
       "      <td>updates Data July Total New Tests K +3922 Case...</td>\n",
       "      <td>(0.0010101010101010073, 0.6015151515151516)</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>0.601515</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>lisacavalerie</td>\n",
       "      <td>None</td>\n",
       "      <td>COVID19Ethiopia</td>\n",
       "      <td>ETHealthData</td>\n",
       "      <td>None</td>\n",
       "      <td>Addis Ababa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1283050980469284864</td>\n",
       "      <td>Tue Jul 14 14:49:26 +0000 2020</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>#covid19ethiopia update \\n—\\nNew cases: 212\\nT...</td>\n",
       "      <td>update New cases Total confirmed cases Active ...</td>\n",
       "      <td>(0.08989898989898991, 0.6681818181818183)</td>\n",
       "      <td>0.089899</td>\n",
       "      <td>0.668182</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Desta_Geb</td>\n",
       "      <td>None</td>\n",
       "      <td>covid19ethiopia</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>Nairobi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1283032624685109253</td>\n",
       "      <td>Tue Jul 14 13:36:29 +0000 2020</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>RT @EthioEmbassyUK: Since the first #COVID19Et...</td>\n",
       "      <td>Since first case reported Govt partners workin...</td>\n",
       "      <td>(0.25, 0.3333333333333333)</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>kobele</td>\n",
       "      <td>None</td>\n",
       "      <td>COVID19Ethiopia</td>\n",
       "      <td>EthioEmbassyUK</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1283015740426203137</td>\n",
       "      <td>Tue Jul 14 12:29:24 +0000 2020</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>RT @EthioEmbassyUK: Since the first #COVID19Et...</td>\n",
       "      <td>Since first case reported Govt partners workin...</td>\n",
       "      <td>(0.25, 0.3333333333333333)</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>jjstruthersuk</td>\n",
       "      <td>None</td>\n",
       "      <td>COVID19Ethiopia</td>\n",
       "      <td>EthioEmbassyUK</td>\n",
       "      <td>None</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1283009108766072833</td>\n",
       "      <td>Tue Jul 14 12:03:03 +0000 2020</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>RT @IOMEthiopia: ✅19,100 returnee migrants rec...</td>\n",
       "      <td>returnee migrants received since April individ...</td>\n",
       "      <td>(0.0, 0.0)</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>OCHA_Ethiopia</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>IOMEthiopia</td>\n",
       "      <td>None</td>\n",
       "      <td>Ethiopia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                      created_at  \\\n",
       "0  1283201041878704134  Wed Jul 15 00:45:43 +0000 2020   \n",
       "1  1283050980469284864  Tue Jul 14 14:49:26 +0000 2020   \n",
       "2  1283032624685109253  Tue Jul 14 13:36:29 +0000 2020   \n",
       "3  1283015740426203137  Tue Jul 14 12:29:24 +0000 2020   \n",
       "4  1283009108766072833  Tue Jul 14 12:03:03 +0000 2020   \n",
       "\n",
       "                                              source  \\\n",
       "0  <a href=\"http://twitter.com/download/android\" ...   \n",
       "1  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "2  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "3  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
       "4  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
       "\n",
       "                                       original_text  \\\n",
       "0  RT @ETHealthData: #COVID19Ethiopia updates \\nD...   \n",
       "1  #covid19ethiopia update \\n—\\nNew cases: 212\\nT...   \n",
       "2  RT @EthioEmbassyUK: Since the first #COVID19Et...   \n",
       "3  RT @EthioEmbassyUK: Since the first #COVID19Et...   \n",
       "4  RT @IOMEthiopia: ✅19,100 returnee migrants rec...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  updates Data July Total New Tests K +3922 Case...   \n",
       "1  update New cases Total confirmed cases Active ...   \n",
       "2  Since first case reported Govt partners workin...   \n",
       "3  Since first case reported Govt partners workin...   \n",
       "4  returnee migrants received since April individ...   \n",
       "\n",
       "                                     sentiment  polarity  subjectivity lang  \\\n",
       "0  (0.0010101010101010073, 0.6015151515151516)  0.001010      0.601515   en   \n",
       "1    (0.08989898989898991, 0.6681818181818183)  0.089899      0.668182   en   \n",
       "2                   (0.25, 0.3333333333333333)  0.250000      0.333333   en   \n",
       "3                   (0.25, 0.3333333333333333)  0.250000      0.333333   en   \n",
       "4                                   (0.0, 0.0)  0.000000      0.000000   en   \n",
       "\n",
       "  favorite_count retweet_count original_author possibly_sensitive  \\\n",
       "0              0             4   lisacavalerie               None   \n",
       "1              1             0       Desta_Geb               None   \n",
       "2              0             5          kobele               None   \n",
       "3              0             5   jjstruthersuk               None   \n",
       "4              0             4   OCHA_Ethiopia               None   \n",
       "\n",
       "          hashtags   user_mentions place place_coord_boundaries  \n",
       "0  COVID19Ethiopia    ETHealthData  None            Addis Ababa  \n",
       "1  covid19ethiopia                  None                Nairobi  \n",
       "2  COVID19Ethiopia  EthioEmbassyUK  None                         \n",
       "3  COVID19Ethiopia  EthioEmbassyUK  None         United Kingdom  \n",
       "4                      IOMEthiopia  None               Ethiopia  "
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mqI5UWTk2lgU"
   },
   "source": [
    "## Stream data and save it to file\n",
    "In the above we saw how to search and fetch data, below we will see how we will stream data from twitter. Make sure you understand the difference between search and stream features of twitter api.\n",
    "\n",
    "### **SAME TASK AS ABOVE**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r6lcy009rX_e"
   },
   "outputs": [],
   "source": [
    "#This is a basic listener that writes received tweets to file.\n",
    "class StdOutListener(StreamListener):\n",
    "\n",
    "    def __init__(self,fhandle, stop_at = 1000):\n",
    "        self.tweet_counter = 0\n",
    "        self.stop_at = stop_at\n",
    "        self.fhandle = fhandle\n",
    "         \n",
    "        \n",
    "    def on_data(self, data):\n",
    "        self.fhandle.write(data)\n",
    "        \n",
    "        #stop if enough tweets are obtained\n",
    "        self.tweet_counter += 1   \n",
    "        if self.tweet_counter < self.stop_at:        \n",
    "            return True\n",
    "        else:\n",
    "            print('Max number of tweets reached: #tweets = ' + str(self.tweet_counter))\n",
    "            return False\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print (status)\n",
    "\n",
    "def stream_tweet_data(filename='data/tweets.json',\n",
    "                      keywords=['COVID19KE'],\n",
    "                      is_async=False):\n",
    "    # tweet topics to use as a filter. The tweets downloaded\n",
    "    # will have one of the topics in their text or hashtag \n",
    "\n",
    "    print('saving data to file: ',filename)\n",
    "\n",
    "    #print the tweet topics \n",
    "    print('Tweet Keywords are: ',keywords)\n",
    "    print('For testing case, please interupt the downloading process \\\n",
    "            using ctrl+x after about 5 mins ')\n",
    "    print('To keep streaming in the background, pass is_async=True')\n",
    "\n",
    "    #Variables that contains the user credentials to access Twitter API \n",
    "    consumer_key = os.environ.get('TWITTER_API_KEY')\n",
    "    consumer_secret = os.environ.get('TWITTER_API_SECRET')\n",
    "    access_token = os.environ.get('TWITTER_ACCESS_TOKEN')\n",
    "    access_token_secret = os.environ.get('TWITTER_ACCESS_TOKEN_SECRET')\n",
    "    \n",
    "\n",
    "    #open file \n",
    "    fhandle=open(filename,'w')\n",
    "\n",
    "    #This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "    l = StdOutListener(fhandle)\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "    stream = Stream(auth, l)\n",
    "\n",
    "    #This line filter Twitter Streams to capture data by the keywords: first argument to this code\n",
    "    stream.filter(track=keywords,is_async=is_async)\n",
    "\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YA9zPpE32lgg"
   },
   "source": [
    "### Use case of the above code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "F8tcPcSMrNLL",
    "outputId": "7badbd2f-2045-42a3-8382-e37f9493f7cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving data to file:  data/covid19_1july2020.json\n",
      "Tweet Keywords are:  ['covid19']\n",
      "For testing case, please interupt the downloading process             using ctrl+x after about 5 mins \n",
      "To keep streaming in the background, pass is_async=True\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-ebc285c3cb79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtweets_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/covid19_1july2020.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstream_tweet_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtweets_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'covid19'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-ddfa844d45ab>\u001b[0m in \u001b[0;36mstream_tweet_data\u001b[0;34m(filename, keywords, is_async)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m#open file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mfhandle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m#This handles Twitter authetification and the connection to Twitter Streaming API\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/covid19_1july2020.json'"
     ]
    }
   ],
   "source": [
    "tweets_file = 'data/covid19_1july2020.json'\n",
    "stream_tweet_data(filename=tweets_file,keywords=['covid19'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GahatJUg2lgq"
   },
   "source": [
    "### Filter twitter data and do basic analysis\n",
    "**Extend it to gain more insight**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gG0AtnCw2lgs"
   },
   "outputs": [],
   "source": [
    "tweets_data = []\n",
    "for line in open(tweets_file, \"r\"):\n",
    "    try:\n",
    "        tweet = json.loads(line)\n",
    "        x=tweet['text']\n",
    "        tweets_data.append(tweet)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "\n",
    "print('saved numbers of tweets: ', len(tweets_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hlFKyGnYrNLX"
   },
   "outputs": [],
   "source": [
    "tweets = pd.DataFrame(columns=['text','lang','country'])\n",
    "\n",
    "tweets['text'] = list(map(lambda tweet: tweet['text'], tweets_data))\n",
    "tweets['lang'] = list(map(lambda tweet: tweet['lang'], tweets_data))\n",
    "tweets['country'] = list(map(lambda tweet: tweet['place']['country'] \\\n",
    "                             if tweet['place'] != None else None, \n",
    "                             tweets_data))\n",
    "\n",
    "\n",
    "tweets_by_lang = tweets['lang'].value_counts()\n",
    "tweets_by_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mVNXsS282lhH"
   },
   "outputs": [],
   "source": [
    "tweets_by_country = tweets['country'].value_counts()\n",
    "tweets_by_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aEPPoCBtrNLd"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.tick_params(axis='x', labelsize=15)\n",
    "ax.tick_params(axis='y', labelsize=10)\n",
    "ax.set_xlabel('Languages', fontsize=15)\n",
    "ax.set_ylabel('Number of tweets' , fontsize=15)\n",
    "ax.set_title('Top 5 languages', fontsize=15, fontweight='bold')\n",
    "tweets_by_lang[:5].plot(ax=ax, kind='bar', color='red', rot=0)\n",
    "\n",
    "tweets_by_country = tweets['country'].value_counts()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.tick_params(axis='x', labelsize=15)\n",
    "ax.tick_params(axis='y', labelsize=10)\n",
    "ax.set_xlabel('Countries', fontsize=15)\n",
    "ax.set_ylabel('Number of tweets' , fontsize=15)\n",
    "ax.set_title('Top 5 countries', fontsize=15, fontweight='bold')\n",
    "tweets_by_country[:5].plot(ax=ax, kind='bar', color='blue', rot=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IQYFg6t5rNLj"
   },
   "source": [
    "# Hashtag histogram. \n",
    "\n",
    "## Please write code that will help you answer the following questions\n",
    " 1) What is the most used hashtag?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4nlNDFpu2lhe"
   },
   "source": [
    " 2) What is the most used referenced username?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "usK167C12lhh"
   },
   "source": [
    " 3) What is the most retweeted tweet?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of twitter_challenge_solution (1).ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
